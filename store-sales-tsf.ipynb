{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Store Sales - Time Series Forecasting\n",
    "---\n",
    "\n",
    "Using machine learning to predict grocery sales\n",
    "\n",
    "In this notebook we will be solving the problem from the competiton: [Store Sales - Time Series Forecasting](https://www.kaggle.com/competitions/store-sales-time-series-forecasting) from [Kaggle](https://www.kaggle.com/)\n",
    "\n",
    "### Summary:\n",
    "*   In this competition, you’ll use time-series forecasting to forecast store sales on data from **Corporación Favorita**, a large Ecuadorian-based grocery retailer.\n",
    "    *   Specifically, you'll build a model that more accurately predicts the unit sales for thousands of items sold at different Favorita stores.  \n",
    "    You'll practice your machine learning skills with an approachable training dataset of dates, store, and item information, promotions, and unit sales.\n",
    "\n",
    "* The evaluation metric for this competition is Root Mean Squared Logarithmic Error - RMSLE.\n",
    "\n",
    "### File Descriptions and Data Field Information\n",
    "\n",
    "#### `train.csv`\n",
    "The training data, comprising time series of features store_nbr, family, and onpromotion as well as the target sales.\n",
    "\n",
    "*   **store_nbr** identifies the store at which the products are sold.\n",
    "*   **family** identifies the type of product sold.\n",
    "*   **sales** gives the total sales for a product family at a particular store at a given date.  \n",
    "    Fractional values are possible since products can be sold in fractional units (1.5 kg of cheese, for instance, as opposed to 1 bag of chips).\n",
    "*   **onpromotion** gives the total number of items in a product family that were being promoted at a store at a given date.\n",
    "\n",
    "#### `test.csv`\n",
    "The test data, having the same features as the training data.  \n",
    "You will predict the target sales for the dates in this file.  \n",
    "*   The dates in the test data are for the 15 days after the last date in the training data.\n",
    "\n",
    "#### `sample_submission.csv`\n",
    "*   A sample submission file in the correct format.\n",
    "\n",
    "#### `stores.csv`\n",
    "Store metadata, including city, state, type, and cluster.\n",
    "*   **cluster** is a grouping of similar stores.\n",
    "\n",
    "#### `oil.csv`\n",
    "Daily oil price.  \n",
    "Includes values during both the train and test data timeframes. (Ecuador is an oil-dependent country and it's economical health is highly vulnerable to shocks in oil prices.)\n",
    "\n",
    "#### `holidays_events.csv`\n",
    "Holidays and Events, with metadata  \n",
    "\n",
    "\n",
    "### NOTES:\n",
    "Pay special attention to the transferred column.  \n",
    "*   A holiday that is transferred officially falls on that calendar day, but was moved to another date by the government.\n",
    "\n",
    "*   A transferred day is more like a normal day than a holiday. To find the day that it was actually celebrated, look for the corresponding row where type is **Transfer**.  \n",
    "    *   For example, the holiday ***Independencia de Guayaquil*** was transferred from **2012-10-09** to **2012-10-12**, which means it was celebrated on **2012-10-12**.  \n",
    "\n",
    "*   Days that are type **Bridge** are extra days that are added to a holiday (e.g., to extend the break across a long weekend).  \n",
    "\n",
    "*   These are frequently made up by the type **Work Day** which is a day not normally scheduled for work (e.g., Saturday) that is meant to payback the **Bridge**.\n",
    "\n",
    "*   Additional holidays are days added a regular calendar holiday, for example, as typically happens around **Christmas** (making Christmas Eve a holiday).\n",
    "\n",
    "*   Wages in the public sector are paid every two weeks on the 15 th and on the last day of the month. **Supermarket sales could be affected by this.**\n",
    "\n",
    "*   A magnitude 7.8 earthquake struck Ecuador on April 16, 2016. People rallied in relief efforts donating water and other first need products which greatly affected supermarket sales for several weeks after the earthquake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries:\n",
    "---\n",
    "\n",
    "To keep the code more concise and undestandable, here I will be listing all of the libraries used during the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System libraries\n",
    "import sys\n",
    "import zipfile\n",
    "import pathlib\n",
    "import os\n",
    "import dotenv\n",
    "import utils\n",
    "\n",
    "# Data Manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Data Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "# Statistical Analysis\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Modelling\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.deterministic import CalendarFourier, DeterministicProcess\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pickle\n",
    "\n",
    "# Warning filter\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_path = pathlib.Path(\"data/train.csv\")\n",
    "if not file_path.is_file():\n",
    "    with zipfile.ZipFile(\"./store-sales-time-series-forecasting.zip\", 'r') as zf:\n",
    "        zf.extractall(\"./data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading all files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign all files to a variable:\n",
    "\n",
    "# sales\n",
    "sales = pd.read_csv('./data/train.csv', dtype={'date':'str'}, parse_dates=['date'])\n",
    "\n",
    "# stores\n",
    "stores = pd.read_csv('./data/stores.csv').rename(columns={'type':'store_type'})\n",
    "\n",
    "# oil prices\n",
    "oil = pd.read_csv('./data/oil.csv', dtype={'date':'str'}, parse_dates=['date']).rename(columns={'dcoilwtico':'oil_price'})\n",
    "\n",
    "# holiday events\n",
    "holidays = pd.read_csv('./data/holidays_events.csv', dtype={'date':'str'}, parse_dates=['date']).rename(columns={'type':'holiday_type'})\n",
    "\n",
    "# transactions\n",
    "transactions = pd.read_csv('./data/transactions.csv', dtype={'date':'str'}, parse_dates=['date'])\n",
    "\n",
    "# train set\n",
    "train = sales.copy()\n",
    "\n",
    "# test set\n",
    "test = pd.read_csv('./data/test.csv', dtype={'date':'str'}, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the head of each file\n",
    "files = {'sales':sales,\n",
    "         'stores':stores,\n",
    "         'oil':oil,\n",
    "         'holidays':holidays,\n",
    "         'transactions':transactions}\n",
    "\n",
    "for filename, data in files.items():\n",
    "    print(f'FILE: {filename}')\n",
    "    display(data.head())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Undestanding the Datasets\n",
    "\n",
    "### 1.1 Shape of the datasets\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "print('Train:', train.shape)\n",
    "\n",
    "# test\n",
    "print('Test:', test.shape, end='\\n\\n')\n",
    "\n",
    "# stores\n",
    "print('Stores:', stores.shape)\n",
    "\n",
    "# transactions\n",
    "print('Transactions:', transactions.shape)\n",
    "\n",
    "# holidays\n",
    "print('Holidays:', holidays.shape)\n",
    "\n",
    "# oil price\n",
    "print('Oil:', oil.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset contains 3,000,888 (99.06% of instances) rows and 6 columns while the test dataset contains 28,512 (00.94% of instances) rows and 5 columns.  \n",
    "\n",
    "\n",
    "The train dataset is **significantly** larger than the test dataset in terms of the number of rows. This is expected (specially in forecasting problems) as the train dataset is usually larger to provide sufficient data for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Features (columns) Information\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_features_info(name:str, df:pd.DataFrame):\n",
    "    print(f'Features Informations for: {name}', end='\\n\\n')\n",
    "    df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {'train':train,\n",
    "              'stores':stores,\n",
    "              'oil':oil,\n",
    "              'holidays':holidays,\n",
    "              'transactions':transactions,\n",
    "              'test':test}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    show_features_info(name, df)\n",
    "    print(end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset contains **3,000,888** entries and 6 columns: 'id', 'date', 'store_nbr', 'family', 'sales', and 'onpromotion'.\n",
    "\n",
    "The test dataset contains **28,512** entries and 5 columns: 'id', 'date', 'store_nbr', 'family', and 'onpromotion'.\n",
    "\n",
    "As expected, the test dataset does not have the \"**sales**\" column. This column is not needed because 'sales' is the feature we want to predict. The goal is to use the trained model to predict or forecast the sales in the test data based on the other available features.\n",
    "\n",
    "*   The Holiday Events Dataset:  \n",
    "The dataset contains 350 entries and 6 columns: 'date', 'type', 'locale', 'locale_name', 'description', and 'transferred'.\n",
    "The \"date\" column in the dataset is of type object. It needs to be converted to a datetime data type for further analysis.\n",
    "\n",
    "\n",
    "*   The Oil Dataset:  \n",
    "The dataset contains 1,218 entries has 2 columns: 'date' and 'oil_price'.\n",
    "The \"date\" column in the dataset is of type object. It needs to be converted to a datetime data type for further analysis.\n",
    "The 'oil_price' column has 1,175 non-null values, indicating that there are some missing values in this column.\n",
    "\n",
    "\n",
    "*   The Stores dataset:  \n",
    "The dataset contains 54 entries and 5 columns: 'store_nbr', 'city', 'state', 'type', and 'cluster'.\n",
    "\n",
    "\n",
    "*   The Transactions dataset:  \n",
    "The dataset contains 83,488 entries and 3 columns: 'date', 'store_nbr', and 'transactions'.\n",
    "The \"date\" column in the dataset is of type object. It needs to be converted to a datetime data type for further analysis.\n",
    "\n",
    "Since we did `pd.read_csv(..., dtype={'date':'str'}, parse_dates=['date'], ...)`, we already have the 'date' column in all datasets with the same type: `datetime64[ns]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Summary Statistics\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_statistics(name:str, df:pd.DataFrame):\n",
    "    if df.select_dtypes(include=[np.int64, np.int32, np.float64, np.float32]).shape[1] > 0:\n",
    "        print(f'Features Statistics for Numerical Data in: {name}', end='\\n\\n')\n",
    "        display(df.select_dtypes(include=[np.int64, np.int32, np.float64, np.float32]).describe().T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {'train':train,\n",
    "              'stores':stores,\n",
    "              'oil':oil,\n",
    "              'holidays':holidays,\n",
    "              'transactions':transactions}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    summary_statistics(name, df)\n",
    "    print(end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the **stores** and **holidays** datasets don't have numerical data:\n",
    "\n",
    "* Stores:\n",
    "\n",
    "    |Column         |Type          |\n",
    "    |---------------|--------------|\n",
    "    |store_nbr      |object        |\n",
    "    |city           |object        |\n",
    "    |state          |object        |        \n",
    "    |type           |object        |        \n",
    "    |cluster        |object        |\n",
    "\n",
    "<br>\n",
    "\n",
    "* Holidays:\n",
    "\n",
    "    |Column         |Type          |\n",
    "    |---------------|--------------|\n",
    "    |date           |datetime64[ns]|\n",
    "    |type           |object        |\n",
    "    |locale         |object        |        \n",
    "    |locale_name    |object        |        \n",
    "    |description    |object        |      \n",
    "    |transferred    |bool          |\n",
    "\n",
    "For the **test** set, we are not going to summarize it, just show the `dtypes`, since we weren't suppose to look at it!\n",
    "\n",
    "* Test:\n",
    "\n",
    "    |Column         |Type          |\n",
    "    |---------------|--------------|\n",
    "    |id             |int64         |\n",
    "    |date           |datetime64[ns]|\n",
    "    |store_nbr      |int64         |\n",
    "    |family         |object        |\n",
    "    |onpromotion    |int64         |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Missing Values\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_missing(name:str, df:pd.DataFrame|pd.Series):\n",
    "    print(f'Missing data for: {name}', end='\\n')\n",
    "    display(df.isnull().sum())\n",
    "    print(\"===\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {'train':train,\n",
    "              'stores':stores,\n",
    "              'oil':oil,\n",
    "              'holidays':holidays,\n",
    "              'transactions':transactions,\n",
    "              'test':test}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    show_missing(name, df)\n",
    "    print(end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the only dataframe that has missing data is the `oil`, so let's focus in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the 'oil_price' column to Identify a Strategy for Handling Missing Values\n",
    "fig = px.line(oil, x='date', y='oil_price')\n",
    "fig.update_layout(title='Trend of Oil Prices Over Time', title_x=0.5, xaxis_title='Date', yaxis_title='Oil Price')\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the missing data occurs randomly on specific days, not on consecutive days. In this case we will use the backfill strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining first and last date\n",
    "first_date = oil['date'].min()\n",
    "last_date = oil['date'].max()\n",
    "\n",
    "# Creating a DatetimeIndex with all dates between start and end, inclusive.\n",
    "all_dates = pd.date_range(start=first_date, end=last_date)\n",
    "\n",
    "# Checking in the training set if all dates are present.\n",
    "missing_dates = all_dates[~all_dates.isin(oil['date'].unique())]\n",
    "if len(missing_dates) == 0:\n",
    "    print(\"There is no missing data. The dataset is complete!\")\n",
    "else:\n",
    "    print(\"There is missing data in the dataset:\")\n",
    "    print(\"Missing Days: \", missing_dates.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the DatetimeIndex of the missing dates in a pandas DataFrame with pd.DataFrame\n",
    "missing_dates = pd.DataFrame({'date':pd.Index(missing_dates, dtype='datetime64[ns]')})\n",
    "\n",
    "# Concatenating this dates to the main training set\n",
    "oil = pd.concat([oil, missing_dates], axis=0, ignore_index=True).sort_values(by='date', ascending=True)\n",
    "oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the bfill (backfill) strategy\n",
    "oil['oil_price'] = oil['oil_price'].bfill()\n",
    "oil.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the 'oil_price' column to Identify a Strategy for Handling Missing Values\n",
    "fig = px.line(oil, x='date', y='oil_price')\n",
    "fig.update_layout(title='Trend of Oil Prices Over Time', title_x=0.5, xaxis_title='Date', yaxis_title='Oil Price')\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Data Completeness\n",
    "---\n",
    "\n",
    "Let's look at the training set and see if all dates between the first and last dates are within the dataset (*data completeness*):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining first and last date\n",
    "first_date = train['date'].min()\n",
    "last_date = train['date'].max()\n",
    "\n",
    "# Creating a DatetimeIndex with all dates between start and end, inclusive.\n",
    "all_dates = pd.date_range(start=first_date, end=last_date)\n",
    "\n",
    "# Checking in the training set if all dates are present.\n",
    "missing_dates = all_dates[~all_dates.isin(train['date'].unique())]\n",
    "if len(missing_dates) == 0:\n",
    "    print(\"There is no missing data. The dataset is complete!\")\n",
    "else:\n",
    "    print(\"There is missing data in the dataset:\")\n",
    "    for i in missing_dates:\n",
    "        print(\"\\t\", i.date())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let input the missing dates in our train dataset, so it can be complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the DatetimeIndex of the missing dates in a pandas DataFrame with pd.DataFrame\n",
    "missing_dates = pd.DataFrame({'date':pd.Index(missing_dates, dtype='datetime64[ns]')})\n",
    "\n",
    "# Concatenating this dates to the main training set\n",
    "train = pd.concat([train, missing_dates], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_dates = all_dates[~all_dates.isin(train['date'].unique())]\n",
    "if len(missing_dates) == 0:\n",
    "    print(\"There is no missing data. The dataset is complete!\")\n",
    "else:\n",
    "    print(\"There is missing data in the dataset:\")\n",
    "    for i in missing_dates:\n",
    "        print(\"\\t\", i.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Merging Datasets\n",
    "\n",
    "Now we will merge all datasets to the training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.1. Segregating Holidays Dataset\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing holidays that were transferred or that happened on weekends\n",
    "useless_days = (holidays['transferred'] == True) | (holidays['holiday_type'] == 'Work Day')\n",
    "tHolidays = holidays.drop(holidays[useless_days].index)\n",
    "tHolidays = tHolidays.drop(['holiday_type', 'description', 'transferred'], axis=1)\n",
    "tHolidays['holiday'] = 1\n",
    "tHolidays = tHolidays.drop(tHolidays[tHolidays['date'].duplicated()].index)\n",
    "\n",
    "# Splitting holidays by 'locale' -> Local, Regional, National\n",
    "local_holidays = tHolidays[tHolidays['locale'] == 'Local']\n",
    "regional_holidays = tHolidays[tHolidays['locale'] == 'Regional']\n",
    "national_holidays = tHolidays[tHolidays['locale'] == 'National'].drop(['locale', 'locale_name'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.2. Merging All Datasets into One\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>store_type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>transactions</th>\n",
       "      <th>holiday</th>\n",
       "      <th>oil_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id       date  store_nbr      family  sales  onpromotion   city      state  \\\n",
       "0   0 2013-01-01          1  AUTOMOTIVE    0.0            0  Quito  Pichincha   \n",
       "1   1 2013-01-01          1   BABY CARE    0.0            0  Quito  Pichincha   \n",
       "2   2 2013-01-01          1      BEAUTY    0.0            0  Quito  Pichincha   \n",
       "3   3 2013-01-01          1   BEVERAGES    0.0            0  Quito  Pichincha   \n",
       "4   4 2013-01-01          1       BOOKS    0.0            0  Quito  Pichincha   \n",
       "\n",
       "  store_type  cluster  transactions  holiday  oil_price  \n",
       "0          D       13           0.0      1.0        NaN  \n",
       "1          D       13           0.0      1.0        NaN  \n",
       "2          D       13           0.0      1.0        NaN  \n",
       "3          D       13           0.0      1.0        NaN  \n",
       "4          D       13           0.0      1.0        NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging with columns ('store_nbr' and 'date') or only ('date') in the datasets using the merge() function with how= 'inner' or 'left', depending.\n",
    "\n",
    "# stores -> train\n",
    "merged = pd.merge(left=train, right=stores, on='store_nbr', how='inner')\n",
    "\n",
    "# transactions -> train\n",
    "merged = pd.merge(left=merged, right=transactions, on=['date', 'store_nbr'], how='left').fillna(0)\n",
    "\n",
    "# national_holidays -> train\n",
    "for date in national_holidays['date']:\n",
    "    merged.loc[merged['date'] == date, 'holiday'] = 1\n",
    "\n",
    "# regional_holidays -> train\n",
    "for date, state in zip(regional_holidays['date'], regional_holidays['locale_name']):\n",
    "    merged.loc[(merged['date'] == date) & (merged['state'] == state), 'holiday'] = 1\n",
    "\n",
    "# local_holidays -> train\n",
    "for date, city in zip(local_holidays['date'], local_holidays['locale_name']):\n",
    "    merged.loc[(merged['date'] == date) & (merged['city'] == city), 'holiday'] = 1\n",
    "\n",
    "# Filling other days of 'holiday' feature with 0\n",
    "merged['holiday'] = merged['holiday'].fillna(0)\n",
    "\n",
    "# oil -> train\n",
    "merged = pd.merge(left=merged, right=oil, on='date', how='left')\n",
    "\n",
    "# View the first five rows of the merged dataset\n",
    "\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.6.2.1 When `inner` or `left` Merge\n",
    "---\n",
    "\n",
    "**Inner Join** (`pd.merge(..., how='inner')`):\n",
    "\n",
    "An inner merge type retains only the rows with matching values in the specified columns. In the context of time series forecasting, it allows us to merge datasets based on a common time index or timestamp. By performing an inner merge, we ensure that only the rows with corresponding timestamps in both datasets are included in the merged result.\n",
    "\n",
    "With an inner merge, you eliminate any non-matching timestamps, which may not be useful for forecasting and could introduce inconsistencies in the data. By focusing on the intersection of the datasets, we can create a merged dataset that contains the necessary information for accurate time series forecasting.\n",
    "\n",
    "<br>\n",
    "\n",
    "**Left Join** (`pd.merge(..., how='left')`):\n",
    "\n",
    "**Use Case**: When you want to keep all rows from the left dataset (e.g., train or test) and only match rows from the right dataset.\n",
    "**Example**: Merging the train dataset with the holidays/events dataset on date, where you want to keep all sales records and only add holiday information when it exists.\n",
    "**When to Use**: Typically used when merging auxiliary data (like store or product information) into the main dataset (like train or test), ensuring that no sales data is dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6.3. Summarizing Merged Data\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_dtypes = [np.int32, np.int64, np.float32, np.float64]\n",
    "\n",
    "# Summary of the merged dataset:\n",
    "print('Summary:')\n",
    "display(merged.select_dtypes(include=numerical_dtypes).describe().T)\n",
    "print(\"===\" * 15, end=\"\\n\\n\")\n",
    "\n",
    "# Checking for missing values:\n",
    "print(\"Missing Data:\")\n",
    "display(merged.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Univariate, Bivariate and Multivariate Analysis\n",
    "\n",
    "### 2.1. Univariate Analysis\n",
    "\n",
    "#### 2.1.1 'Sales' feature:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(merged['sales'], bins=20)\n",
    "plt.xlabel('Sales')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Sales')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.boxplot(merged['sales'])\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Boxplot of Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram and boxplot of the 'sales' feature provide insights into its distribution.  \n",
    "The histogram shows the frequency distribution of sales values. It reveals that the majority of sales fall within a specific range, represented by the peak in the histogram. However, there are also instances of higher sales values, leading to a right-skewed distribution.  \n",
    "\n",
    "This skewness suggests that there are relatively fewer occurrences of very high sales, while the majority of sales are concentrated around lower values. The boxplot further confirms the presence of outliers in the data, as indicated by the points beyond the markers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 'transactions' Feature:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(merged['transactions'], bins=20)\n",
    "plt.xlabel('Transactions')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Transactions')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.boxplot(merged['transactions'])\n",
    "plt.ylabel('Transactions')\n",
    "plt.title('Boxplot of Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This histogram provides insights into the distribution of transactions in the dataset.  \n",
    "The shape of the histogram indicates that the majority of transactions fall into a specific range, which is evident from the high frequency observed on the left side of the histogram. As the transactions increase, the frequency gradually decreases, forming a right-skewed distribution. This suggests that there are relatively fewer instances of high transaction volumes.  \n",
    "\n",
    "Overall, the histogram highlights the presence of a cluster of transactions with a lower frequency, indicating a pattern in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 'oil_price' Feature:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(merged['oil_price'], bins=20)\n",
    "plt.xlabel('Oil Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Oil Price')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.boxplot(merged['oil_price'])\n",
    "plt.ylabel('Oil Price')\n",
    "plt.title('Boxplot of Oil Price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram and boxplot of the 'oil_price' feature reveal insights into its distribution.  \n",
    "The histogram displays the frequency distribution of oil prices, indicating the number of occurrences for each price range.  \n",
    "\n",
    "As we can see in the boxplot, there is no outlier in the 'oil_price' column, probably because oil is a commodity and its value is fixed at a certain margin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 'onpromotion' Feature:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram\n",
    "plt.hist(merged['onpromotion'], bins=20)\n",
    "plt.xlabel('On Promotion Sales')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of OnPromotion Sales')\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.boxplot(merged['onpromotion'])\n",
    "plt.ylabel('On Promotion Sales')\n",
    "plt.title('Boxplot of On Promotion Sales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram and boxplot of the 'onpromotion' feature provide insights into its distribution.  \n",
    "The histogram shows the frequency distribution of onpromotion values. It reveals that the majority of sales fall within a specific range, represented by the peak in the histogram. However, like the 'sales' distribution, there are also instances of higher sales values, leading to a right-skewed distribution.  \n",
    "\n",
    "This skewness suggests that there are relatively fewer occurrences of very high sales, while the majority of sales are concentrated around lower values. The boxplot further confirms the presence of outliers in the data, as indicated by the points beyond the markers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Bivariate Analysis\n",
    "\n",
    "#### 2.2.1 Trend of 'sales' Feature Over Time:\n",
    "---\n",
    "\n",
    "<a id='sales-over-time'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by date and calculate the total sales\n",
    "daily_sales = merged.groupby('date')['sales'].sum().reset_index()\n",
    "\n",
    "# Create a time series plot with slider\n",
    "fig = px.line(daily_sales, x='date', y='sales')\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.update_layout(title='Trend of Sales Over Time', title_x=0.5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows the trend of sales over time.  \n",
    "From the graph, we can observe that the sales exhibit some variations and fluctuations over time. There are periods of both high and low sales, indicating potential seasonality or other factors affecting sales patterns.\n",
    "\n",
    "Special attention for the end of December and begin of January of each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2. Oil Price ('oil_price') Feature Over Time:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the 'oil_price' column to confirm if the trend is consistent.\n",
    "fig = px.line(oil, x='date', y='oil_price')\n",
    "fig.update_layout(title='Trend of Oil Prices Over Time', title_x=0.5, xaxis_title='Date', yaxis_title='Oil Price')\n",
    "fig.update_xaxes(rangeslider_visible=True)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the oil prices over time, we notice a clear trend. Oil prices experienced a significant drop towards the end of 2014 and have not bounced back since. Despite some fluctuations, they remain at the same level as they were at the start of 2015.  \n",
    "\n",
    "Consequently, we might expect a notable change in store sales around late 2014. However, this isn’t immediately obvious when examining the unit sales data ([see 2.2.1](#sales-over-time)). While sales do seem to decline in early 2015, they actually increase in late 2014. Additionally, the drop in oil prices does not appear to affect sales, as shown in the sales plot. There is no correlation between the decrease in oil prices and sales, indicating that this data is not relevant for our modeling purposes and will be excluded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.3. Sales ('sales') by Store Type ('store_type')\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the total sales by store_type:\n",
    "store_type_sales = merged.groupby('store_type')['sales'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Create a bar plot with \"flare_r\" color palette for total sales\n",
    "sns.set_palette(\"flare_r\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=store_type_sales.index, y=store_type_sales.values)\n",
    "plt.xlabel('Store Type')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Total Sales by Store Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total sales amount varies across different store types.  \n",
    "Store Type `D` has the highest total sales, indicating its significant contribution to the overall sales. Store Type `A` follows closely behind, demonstrating its substantial sales performance.  \n",
    "Store Type C ranks third in terms of total sales, while Store Type B and Store Type E have lower sales amounts.\n",
    "\n",
    "Understanding the variations in sales by store type helps identify the key drivers of revenue and highlights the importance of certain store types in driving overall sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.4. Total Count per Store Type:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the total sales by store_type:\n",
    "store_type_counts = merged['store_type'].value_counts().sort_values(ascending=False)\n",
    "\n",
    "# Create a bar plot with \"flare_r\" color palette for total count\n",
    "sns.set_palette(\"flare_r\")\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=store_type_counts.index, y=store_type_counts.values)\n",
    "plt.xlabel('Store Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Total Count by Store Type')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis of the total count of sales by store type provides insights into the sales performance and customer demand across different store types. Store Type D stands out with the highest count of sales, suggesting a strong customer base and popularity of products offered. Store Type C follows with a relatively lower count of sales, indicating a significant customer base as well.\n",
    "\n",
    "On the other hand, Store Types A, B, and E have lower counts, suggesting potential areas for improvement or the need to address competition.\n",
    "\n",
    "Special attention for Store Type `A`, since, as shown above, has the second highest total amount of sales but ranks third in the total count of sales in the set, problably indicating that a good portion of Store Type `A` sales were made in a single day!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.5. Average Sales ('sales') by City\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sales_by_city = merged.groupby('city')['sales'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Create a bar plot with \"flare_r\" color palette for total sales\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(y=avg_sales_by_city.index, x=avg_sales_by_city.values, orient='h', palette='flare_r')\n",
    "plt.xlabel('Average Sales')\n",
    "plt.ylabel('City')\n",
    "plt.title('Average Sales by City')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quito has the highest number of stores, significantly surpassing other cities. Cayambe is the second most populated city in terms of stores followed by Ambato, Cuenca and Daule. Some cities have a moderate number of stores while others have a lower number of stores. Playas has the lowest number of average sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.6. Sales ('sales') x Transactions ('transactions')\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data= merged, x='transactions', y='sales', palette='flare_r')\n",
    "plt.xlabel('Transactions')\n",
    "plt.ylabel('Sales')\n",
    "plt.title('Relationship between Sales and Transactions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scatter plot illustrates the connection between sales and transactions within the dataset. Each point on the plot represents a specific instance with its corresponding sales and transaction values. Here are the main insights derived from the scatter plot:\n",
    "\n",
    "Clustered Data Points: Most data points are concentrated in the lower sales region, forming a distinct cluster. This suggests that certain transaction volumes are consistently linked with specific sales levels, indicating a common sales pattern or trend at these transaction levels.\n",
    "\n",
    "Outliers: Some data points deviate from the main cluster, appearing at higher sales levels for relatively lower transaction volumes or vice versa. These outliers represent exceptional cases where sales significantly differ from the typical pattern for a given number of transactions. Understanding these outliers can provide valuable insights into unusual sales scenarios or exceptional business activities.\n",
    "\n",
    "In summary, the scatter plot offers valuable insights into the relationship between sales and transactions. The clustering of data points around specific sales and transaction levels reveals common patterns, while outliers highlight exceptional cases that merit further investigation. This analysis can help businesses make informed decisions and develop effective strategies to enhance sales performance.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Multivariate Analysis\n",
    "\n",
    "#### 2.3.1. Correlation Matrix\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for numerical data\n",
    "correlation_matrix = merged.select_dtypes(['int64', 'int32', 'float64', 'float32']).drop(columns=('id')).corr()\n",
    "\n",
    "# Heatmap\n",
    "sns.heatmap(correlation_matrix, cmap='flare', annot=True)\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation values range from `-1` to `1`, where -1 indicates a perfect negative correlation, 1 indicates a perfect positive correlation, and 0 indicates no correlation. The table helps us understand the relationships between variables, providing valuable insights into their interactions. In this correlation matrix, we observe the correlations between different variables:\n",
    "\n",
    "Sales x Transactions: There is a weak positive correlation of approximately `0.23` between “Sales” and “Transactions.” This suggests a slight positive relationship between the number of transactions and sales. It implies that as the number of transactions increases, sales tend to increase as well, although the correlation is not very strong.\n",
    "\n",
    "Sales x Oil Price: There is a weak negative correlation of approximately `-0.075` between “Sales” and “Dcoilwito” (Oil Prices). This indicates a slight negative relationship between sales and oil prices. It suggests that as oil prices rise, sales tend to decrease slightly, though the correlation is not significant.\n",
    "\n",
    "Sales x Onpromotion: There is a high positive correlation of approximately `0.43` between “sales” and “onpromotion”. This is to be expected, since \"onpromotions\" represents the number of sales that were made during a promotion. So, in general, as products are sold during a promotion, sales tend to rise.\n",
    "\n",
    "Transactions x Oil Price: There is a very weak negative correlation of approximately `-0.052` between “Transactions” and “Dcoilwito” (Oil Prices). This suggests almost no relationship between the number of transactions and oil prices. It indicates that fluctuations in oil prices do not significantly impact the number of transactions.\n",
    "\n",
    "Overall, the correlation values are relatively low, indicating that the relationships between these variables are not very strong. Other factors not considered in this correlation matrix may also influence sales, transactions, and oil prices. It’s essential to explore additional factors to gain a more comprehensive understanding of their impact on sales and transactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. Pair Plot\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3.2.1. What is a Pair Plot?\n",
    "A pair plot is a type of data visualization that plots pairwise relationships between variables in a dataset. It helps you understand the interactions and correlations between different variables by displaying them in a grid of scatter plots and histograms.\n",
    "\n",
    "##### 2.3.2.2. Key Features of `pairplot`:\n",
    "1. **Grid of Plots**:\n",
    "   - The `pairplot` function creates a grid where each variable in the dataset is plotted against every other variable.\n",
    "   - The diagonal of the grid shows the univariate distribution of each variable, typically using histograms or kernel density estimates (KDE).\n",
    "\n",
    "2. **Customization**:\n",
    "   - You can customize the appearance of the plots using various parameters, such as `hue` to color-code data points based on a categorical variable, `kind` to specify the type of plot (e.g., scatter, KDE, or regression), and `diag_kind` to choose the type of plot for the diagonal.\n",
    "\n",
    "3. **Highlighting Relationships**:\n",
    "   - By visualizing all pairwise relationships, you can easily spot trends, correlations, and outliers in the data.\n",
    "   - This is particularly useful for exploratory data analysis (EDA) to get a quick overview of the dataset.\n",
    "\n",
    "##### 2.3.2.3. Benefits of Using `pairplot`:\n",
    "- **Comprehensive Visualization**: Provides a comprehensive view of pairwise relationships in the dataset.\n",
    "- **Easy Customization**: Offers various customization options to tailor the plots to your needs.\n",
    "- **Quick Insights**: Helps quickly identify patterns, correlations, and outliers.\n",
    "\n",
    "Overall, `pairplot` is a versatile and powerful tool for visualizing and exploring the relationships between variables in a dataset¹²³."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for pair plot\n",
    "features_pairplot = ['sales', 'onpromotion', 'transactions', 'oil_price']\n",
    "\n",
    "# Plot scatter plot matrix\n",
    "sns.pairplot(data=merged[features_pairplot])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistical Tests\n",
    "\n",
    "### 3.1. Stationarity\n",
    "---\n",
    "\n",
    "In statistics, stationarity refers to a property of a time series where its statistical characteristics, such as **mean, variance, and autocorrelation**, remain constant over time.  \n",
    "This consistency makes it easier to model and predict the series.\n",
    "\n",
    "For our project, we will be using the the Augmented Dickey-Fuller (ADF) test.  \n",
    "The the Augmented Dickey-Fuller (ADF) test is a statistical test used to determine whether a time series dataset is stationary or not.\n",
    "\n",
    "O teste Augmented Dickey-Fuller (ADF) é um teste estatístico usado para determinar se uma série temporal é estacionária, ou seja, se suas propriedades estatísticas (como a média, a variância e a autocorrelação) são constantes ao longo do tempo. A estacionariedade é uma suposição importante em muitos modelos de séries temporais, como o ARIMA.\n",
    "\n",
    "The ADF equation:\n",
    "\n",
    "$\n",
    "\\Delta y_t = \\alpha + \\beta t + \\gamma y_{t-1} + \\delta_1 \\Delta y_{t-1} + \\delta_2 \\Delta y_{t-2} + \\dots + \\delta_p \\Delta y_{t-p} + \\epsilon_t\n",
    "$\n",
    "\n",
    "Where:\n",
    "- $ \\Delta y_t $ is the difference of the series $ y_t $,\n",
    "- $ t $ is the time,\n",
    "- $ \\gamma y_{t-1} $ represents the term of the unitary root,\n",
    "- $ \\delta $ are the coefficients of the delayed difference,\n",
    "- $ \\epsilon_t $ is the error.\n",
    "\n",
    "The hypothesis:\n",
    "*   Null hypothesis (H0): The sales data is non-stationary.\n",
    "*   Alternative hypothesis (H1): The sales data is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical Test of the 'sales' column\n",
    "sales_data = merged[['date', 'sales']].groupby('date').sum()['sales']\n",
    "\n",
    "# Perform ADF test using adfuller from statsmodels.tsa.stattools\n",
    "result = adfuller(sales_data)\n",
    "\n",
    "# Extract the test statistics and p-value from the result\n",
    "test_statistic = result[0]\n",
    "p_value = result[1]\n",
    "critical_values = result[4]\n",
    "\n",
    "# Print the test statistics and critical values\n",
    "print(f\"ADF Test Statistics: {test_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "print(\"Critical Values:\")\n",
    "for key, value in critical_values.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "# Check the p-value against a significance level (e.g., 0.05)\n",
    "if p_value <= 0.05:\n",
    "    print(\"Reject the null hypothesis: The sales data is stationary.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The sales data is non-stationary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the ADF test, the test statistics (-2.616) is lower than the critical values (10%) and higher in other confidence levels (1% and 5%).  \n",
    "Additionally, the p-value is `0.0896`, which is higher than the significance level of `0.05`.\n",
    "\n",
    "Since the `p-value` is more than `0.05`, we fail to reject the null hypothesis, indicating that the sales data is non-stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Do Promotion has Impact on Sales?\n",
    "---\n",
    "\n",
    "To answer this, we will perform a two-sample **t-test** to compare sales between promotional (`'onpromotion' > 0.0`) and non-promotional (`'onpromotion' == 0.0`) periods.\n",
    "\n",
    "The Hypothesis:\n",
    "*   Null Hypothesis (H0): The promotional activities does not have a significant impact on store sales.\n",
    "*   Alternative Hypothesis (H1): The promotional activities have a significant impact on store sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the variables for the hypothesis test\n",
    "promo_sales = merged[merged['onpromotion'] > 0.].groupby('date').sum()['sales']\n",
    "non_promo_sales = merged[merged['onpromotion'] == 0.].groupby('date').sum()['sales']\n",
    "\n",
    "# Perform a two-sample t-test to compare sales between promotional and non-promotional periods\n",
    "t_statistic, p_value = ttest_ind(promo_sales, non_promo_sales)\n",
    "\n",
    "# Print the test result\n",
    "print(\"Hypothesis Testing for Promotional Activities:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Test Statistic:\", t_statistic)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"=\" * 50)\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis. Promotional activities have a significant impact on store sales.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. Promotional activities do not have a significant impact on store sales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the hypothesis test, we obtained a very low p-value of almost `0.0`.  \n",
    "This indicates strong evidence to reject the null hypothesis. Therefore, we can conclude that promotional activities have a significant impact on store sales.  \n",
    "\n",
    "The test statistic of ~`208.4` also suggests a substantial difference in sales between promotional and non-promotional periods.  \n",
    "These results support the notion that promotional activities play a crucial role in driving store sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Do the Earthquake (April 16, 2016) Impacted Sales?\n",
    "---\n",
    "To verify that, we will do a hypothesis test and a plot:\n",
    "\n",
    "*   The Plot:\n",
    "    *   Shows the sales before and after the earthquake\n",
    "\n",
    "*   The Hypothesis:\n",
    "    *   Null Hypothesis (H0): The earthquake does not had a significant impact on store sales.\n",
    "    *   Alternative Hypothesis (H1): The earthquake had a significant impact on store sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the variable earthquake_date to the date the earthquake took place (April 16, 2016)\n",
    "earthquake = pd.to_datetime('2016-04-16')\n",
    "\n",
    "# Creating total_sales, which is the sum of sales for each day\n",
    "total_sales = train.groupby('date')['sales'].sum()\n",
    "mean_sales = train.groupby('date')['sales'].mean()\n",
    "\n",
    "# Filter the sales data before and after the earthquake\n",
    "sum_sales_before_earthquake = total_sales[total_sales.index < earthquake]\n",
    "sum_sales_after_earthquake = total_sales[total_sales.index > earthquake]\n",
    "mean_sales_before_earthquake = mean_sales[mean_sales.index < earthquake]\n",
    "mean_sales_after_earthquake = mean_sales[mean_sales.index > earthquake]\n",
    "\n",
    "# Set the colormap to viridis\n",
    "colormap = cm.get_cmap('viridis')\n",
    "\n",
    "# Plot the sales before and after the earthquake\n",
    "plt.plot(sum_sales_before_earthquake, color=colormap(0.5), label='Sales Before Earthquake')\n",
    "plt.plot(sum_sales_after_earthquake, color=colormap(0.9), label='Sales After Earthquake')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.title('Total Sales: Before x After Earthquake')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(mean_sales_before_earthquake, color=colormap(0.5), label='Sales Before Earthquake')\n",
    "plt.plot(mean_sales_after_earthquake, color=colormap(0.9), label='Sales After Earthquake')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sales')\n",
    "plt.legend()\n",
    "plt.title('Mean Sales: Before x After Earthquake')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "import datetime\n",
    "\n",
    "day_30_before_earthquake = earthquake - datetime.timedelta(days=30)\n",
    "day_30_after_earthquake = earthquake + datetime.timedelta(days=30)\n",
    "\n",
    "mean_sales_before = mean_sales_before_earthquake[\n",
    "    (mean_sales_before_earthquake.index < earthquake) & (mean_sales_before_earthquake.index >= day_30_before_earthquake)\n",
    "    ]\n",
    "mean_sales_after = mean_sales_after_earthquake[\n",
    "    (mean_sales_after_earthquake.index > earthquake) & (mean_sales_after_earthquake.index <= day_30_after_earthquake)\n",
    "    ]\n",
    "\n",
    "# Perform a two-sample t-test to compare sales between promotional and non-promotional periods\n",
    "t_statistic, p_value = ttest_rel(mean_sales_before, mean_sales_after)\n",
    "\n",
    "# Print the test result\n",
    "print(\"Hypothesis Testing for Earthquake impact on Sales:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"Test Statistic:\", t_statistic)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"=\" * 50)\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis. Earthquake had a significant impact on store sales.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. Earthquake do not had a significant impact on store sales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing with 30 days sales before the earthquake and 30 days sales after the earthquake, we got the P-value of `0.02`, so we reject the null hypothesis. Earthquake had a significant impact on store sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Are Clusters of Stores Different in Sales?\n",
    "---\n",
    "\n",
    "For this type of analysis we will use ANOVA, since we have more than 2 groups (clusters), and we want to compare if their means are different.\n",
    "\n",
    "To verify that, we will do a hypothesis test and a plot:\n",
    "\n",
    "*   The Plot:\n",
    "    *   Shows the sales per cluster.\n",
    "\n",
    "*   The Hypothesis:\n",
    "    *   Null Hypothesis (H0): The cluster have no impact on sales.\n",
    "    *   Alternative Hypothesis (H1): The cluster have impact on sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by cluster and calculate the average sales and sort in descending order\n",
    "average_sales_by_cluster = merged.groupby('cluster')['sales'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Set the number of bars in each plot\n",
    "num_bars = len(average_sales_by_cluster)\n",
    "\n",
    "# Generate the colors using the viridis palette\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, num_bars))\n",
    "\n",
    "# Plot the average sales by cluster\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(average_sales_by_cluster.index, average_sales_by_cluster.values, color=colors)\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.title('Average Sales by Cluster')\n",
    "\n",
    "# Set the x-tick labels as integers\n",
    "plt.xticks(range(1, len(average_sales_by_cluster.index)+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Group by date and cluster and calculates the sum of sales, then unstack cluster, having 1 column of sales for each cluster.\n",
    "sum_sales_by_cluster = merged.loc[:, ['date', 'cluster', 'sales']].groupby(['date', 'cluster']).sum().unstack('cluster').fillna(0.)\n",
    "list_sales_cluster = [sum_sales_by_cluster.iloc[:, cluster] for cluster in range(sum_sales_by_cluster.shape[1])]\n",
    "\n",
    "# ANOVA\n",
    "f_statistic, p_value = stats.f_oneway(\n",
    "    list_sales_cluster[0],\n",
    "    list_sales_cluster[1],\n",
    "    list_sales_cluster[2],\n",
    "    list_sales_cluster[3],\n",
    "    list_sales_cluster[4],\n",
    "    list_sales_cluster[5],\n",
    "    list_sales_cluster[6],\n",
    "    list_sales_cluster[7],\n",
    "    list_sales_cluster[8],\n",
    "    list_sales_cluster[9],\n",
    "    list_sales_cluster[10],\n",
    "    list_sales_cluster[11],\n",
    "    list_sales_cluster[12],\n",
    "    list_sales_cluster[13],\n",
    "    list_sales_cluster[14],\n",
    "    list_sales_cluster[15],\n",
    "    list_sales_cluster[16],\n",
    ")\n",
    "\n",
    "# Print the test result\n",
    "print(\"Hypothesis Testing for Cluster Sales:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"F Statistic:\", f_statistic)\n",
    "print(\"P-value:\", p_value)\n",
    "print(\"=\" * 50)\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis. Cluster have a significant impact on store sales.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis. Cluster do not have a significant impact on store sales.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just by analyzing the graph of average sales per cluster, we were already able to draw relevant information.\n",
    "\n",
    "With the F-test we were able to confirm that the different clusters have different amounts of sales over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Do Product Families Behave Differently in Terms of Sales?\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the data by product family and calculate the total sales\n",
    "family_sales = merged.groupby('family')['sales'].sum().sort_values(ascending=False)\n",
    "\n",
    "# Select the top 10 product families\n",
    "top_10_families = family_sales.head(10)\n",
    "\n",
    "# Plot product family x sales for the top 10 families\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=top_10_families.index, y=top_10_families.values, palette='flare_r')\n",
    "plt.xlabel('Product Family')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.title('Total Sales by Product Family (Top 10)')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there is a big difference in terms of sales between the product families."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6. Do Store Number ('store_nbr') Behave Differently in Terms of Sales?\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the sum of sales for each store_nbr\n",
    "store_sales = merged.groupby('store_nbr')['sales'].sum()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "ax = sns.barplot(x=store_sales.index, y=store_sales.values, palette='flare_r')\n",
    "plt.xlabel('Store Number')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.title('Sales by Store Number')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, there is a great difference in terms of sales between each store number ('store_nbr')."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 4. Feature Engineering\n",
    "\n",
    "### 4.1. What Features Can We Extract From Date?\n",
    "---\n",
    "\n",
    "In this part, we will extract year, month and day from the date feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a copy of the merged dataframe\n",
    "merged_copy = merged.copy()\n",
    "\n",
    "# Extracting Date Parts\n",
    "merged_copy['date'] = pd.to_datetime(merged_copy['date'])\n",
    "merged_copy['year'] = merged_copy['date'].dt.year\n",
    "merged_copy['month'] = merged_copy['date'].dt.month\n",
    "merged_copy['day'] = merged_copy['date'].dt.day\n",
    "merged_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set distinct colors for each year\n",
    "colors = sns.color_palette(\"husl\", n_colors=len(merged_copy['year'].unique()))\n",
    "\n",
    "months = ['Jan','Feb', 'Mar',\n",
    "          'Apr', 'May', 'Jun',\n",
    "          'Jul', 'Aug', 'Sep',\n",
    "          'Oct', 'Nov', 'Dec']\n",
    "\n",
    "# Visualize the Monthly Sales\n",
    "monthly_sales = merged_copy.groupby(['year', 'month'])['sales'].sum().reset_index()\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(data=monthly_sales, x='month', y='sales', hue='year', palette=colors)\n",
    "plt.title('Monthly Sales Trend')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.xticks(range(1, 13), labels=months)\n",
    "plt.legend(title='Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line plot depicting the monthly sales shows variations in sales throughout the year.  \n",
    "Sales seem to dip during the middle months of the year (July to September) before rising again in the last quarter (October to December), with the highest sales in December.  \n",
    "The year-wise color distinction helps to observe sales patterns for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Yearly Sales Trend\n",
    "yearly_sales = merged_copy.groupby('year')['sales'].sum().reset_index()\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=yearly_sales, x='year', y='sales', palette=colors)\n",
    "plt.title('Yearly Sales Trend')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Total Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bar plot shows the total sales for each year. As we can see, 2016 had the hightest sum of sales among the 5 years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the Day of the Week Sales Pattern\n",
    "days_of_week = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "merged_copy['day_of_week'] = merged_copy['date'].dt.dayofweek\n",
    "day_of_week_sales = merged_copy.groupby('day_of_week')['sales'].mean().reset_index()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(data=day_of_week_sales, x='day_of_week', y='sales', palette='flare_r')\n",
    "plt.title('Average Sales by Day of the Week')\n",
    "plt.xlabel('Day of the Week')\n",
    "plt.ylabel('Average Sales')\n",
    "plt.xticks(range(7), labels=days_of_week)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, in Saturday and Sunday there is no sales, because the stores don't operate on weekend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Mutual Information\n",
    "\n",
    "#### 4.2.1. MI for Numerical Features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate target and features for Mutual Information\n",
    "# X = merged_copy.drop('sales', axis=1).select_dtypes(include=numerical_dtypes)\n",
    "# y = merged_copy['sales']\n",
    "\n",
    "# # Fit the mutual_info_regression\n",
    "# mi = mutual_info_regression(X, y, random_state=42)\n",
    "\n",
    "# # Showing the results\n",
    "# print(\"Mutual Information for Numeric Features:\")\n",
    "# print('---'*13, end=\"\\n\\n\")\n",
    "# for feature, value in zip(X.columns, mi):\n",
    "#     print(f\"{feature}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the numerical features there are two highlights:\n",
    "*   `onpromotion`: MI ~0.212;\n",
    "*   `transactions`: MI ~0.147;\n",
    "*   `store_nbr`: MI ~0.134.\n",
    "\n",
    "The `id` feature is a counter that goes from `0` to `len(n)-1`. It has no useful information for our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2. MI for Categorical Features\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate target and features for Mutual Information\n",
    "# X = merged_copy.drop('sales', axis=1).select_dtypes(include=['category', 'object'])\n",
    "# for non_cat in X.select_dtypes(include=['object']).columns:\n",
    "#     X[non_cat] = X[non_cat].astype('category')\n",
    "\n",
    "# y = merged_copy['sales']\n",
    "\n",
    "# # Encode features with values between 0 and n_categories-1\n",
    "# ord_enc = OrdinalEncoder()\n",
    "# col_names = X.columns\n",
    "# X = ord_enc.fit_transform(X)\n",
    "# X = pd.DataFrame(data=X, columns=col_names)\n",
    "\n",
    "# # Fit the mutual_info_regression\n",
    "# mi = mutual_info_regression(X, y, random_state=42)\n",
    "\n",
    "# # Showing the results\n",
    "# print(\"Mutual Information for Categorical Features:\")\n",
    "# print('---'*13, end=\"\\n\\n\")\n",
    "# for feature, value in zip(X.columns, mi):\n",
    "#     print(f\"{feature}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the `family` feature has a very important impact on `sales` (target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### 4.3. Scaling Features:\n",
    "\n",
    "#### 4.3.1 StandardScaler\n",
    "---\n",
    "Standardize features by removing the mean and scaling to unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create instance of StandardScaler\n",
    "# std_scaler = StandardScaler()\n",
    "\n",
    "# # Select Numerical features\n",
    "# numerical_features = ['onpromotion', 'transactions', 'oil_price']\n",
    "\n",
    "# # Fit the numerical columns and then Tranform them inplace\n",
    "# merged_copy[numerical_features] = std_scaler.fit_transform(merged_copy[numerical_features])\n",
    "\n",
    "# # Display the updated dataframe\n",
    "# merged_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we won't scale the target `sales`, because the metric used in the competition is **RMSLE**.  \n",
    "Here is why:\n",
    "\n",
    "*   **RMSLE is Scale-Invariant**:\n",
    "    *   RMSLE naturally deals with differences in scale because it is based on the ratio between predicted and actual values (after taking the logarithm). This makes it scale-invariant, meaning it won't directly benefit from scaling the target variable.\n",
    "    If you scale your target variable, the logarithmic transformation in RMSLE will simply reverse that scaling.\n",
    "\n",
    "*   **Logarithmic Transformation**:\n",
    "    *    The logarithmic function applied within RMSLE already compresses large ranges into a more manageable scale. Scaling the target before applying RMSLE might not have the intended effect because the log transformation already normalizes the differences.\n",
    "\n",
    "*   **Interpretation Issues**:\n",
    "    *   If you scale the target variable before calculating RMSLE, the resulting metric will no longer be directly interpretable in the original units. This can complicate the evaluation and comparison of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### 4.4. Encoding Features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.1. Drop Unneccessary Features:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Unneccessary Features\n",
    "columns_to_drop = ['date','id', 'store_type', 'state']\n",
    "\n",
    "# Droping Features\n",
    "merged_copy = merged_copy.drop(columns=columns_to_drop)\n",
    "\n",
    "# Print first 5 rows\n",
    "merged_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.2. Product Categorization Based on Families\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values of 'family' feature and print it\n",
    "families = merged_copy['family'].unique()\n",
    "families"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create groups based on similar families\n",
    "# eletronic_families = ['PLAYERS AND ELECTRONICS','HARDWARE']\n",
    "# clothing_families = ['LINGERIE', 'LADYSWARE']\n",
    "# food_families = ['BEVERAGES', 'BREAD/BAKERY', 'FROZEN FOODS', 'MEATS', 'PREPARED FOODS', 'DELI','PRODUCE', 'DAIRY','POULTRY','EGGS','SEAFOOD']\n",
    "# grocery_families = ['GROCERY I', 'GROCERY II']\n",
    "# cleaning_families = ['HOME CARE', 'BABY CARE','PERSONAL CARE']\n",
    "# home_families = ['HOME AND KITCHEN I', 'HOME AND KITCHEN II', 'HOME APPLIANCES']\n",
    "# stationery_families = ['BOOKS', 'MAGAZINES','SCHOOL AND OFFICE SUPPLIES']\n",
    "\n",
    "\n",
    "# # Categorize the 'family' feature based on the family groups\n",
    "# merged_copy['family'] = np.where(merged_copy['family'].isin(food_families), 'FOODS', merged_copy['family'])\n",
    "# merged_copy['family'] = np.where(merged_copy['family'].isin(home_families), 'HOME', merged_copy['family'])\n",
    "# merged_copy['family'] = np.where(merged_copy['family'].isin(clothing_families), 'CLOTHING', merged_copy['family'])\n",
    "# merged_copy['family'] = np.where(merged_copy['family'].isin(grocery_families), 'GROCERY', merged_copy['family'])\n",
    "# merged_copy['family'] = np.where(merged_copy['family'].isin(stationery_families), 'STATIONERY', merged_copy['family'])\n",
    "# merged_copy['family'] = np.where(merged_copy['family'].isin(cleaning_families), 'CLEANING', merged_copy['family'])\n",
    "# merged_copy['family'] = np.where(merged_copy['family'].isin(eletronic_families), 'ELETRONICS', merged_copy['family'])\n",
    "\n",
    "# # Print the updated DataFrame\n",
    "# merged_copy.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "families = merged_copy['family'].unique()\n",
    "families"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4.3. OneHotEncoding Categorical Features:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot(df, cat_features):\n",
    "    encoder = OneHotEncoder()\n",
    "    one_hot_encoded_data = encoder.fit_transform(df[cat_features])\n",
    "\n",
    "    # Create column names for the one-hot encoded data\n",
    "    column_names = encoder.get_feature_names_out(cat_features)\n",
    "\n",
    "    # Convert the one-hot encoded data to a DataFrame\n",
    "    merged_encoded = pd.DataFrame(one_hot_encoded_data.toarray(), columns=column_names)\n",
    "\n",
    "    # Concatenate the original dataframe with the one-hot encoded data\n",
    "    merged_encoded = pd.concat([df, merged_encoded], axis=1)\n",
    "\n",
    "    # Drop the original categorical columns\n",
    "    merged_encoded.drop(cat_features, axis=1, inplace=True)\n",
    "\n",
    "    # Return the encoded DataFrame\n",
    "    return merged_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select categorical features\n",
    "# categorical_features = merged_copy.select_dtypes(include=['category', 'object']).columns.to_list()\n",
    "categorical_features = [\"family\", \"city\"]\n",
    "\n",
    "# Perform one-hot encoding\n",
    "encoder = OneHotEncoder()\n",
    "one_hot_encoded_data = encoder.fit_transform(merged_copy[categorical_features])\n",
    "\n",
    "# Create column names for the one-hot encoded data\n",
    "column_names = encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "# Convert the one-hot encoded data to a DataFrame\n",
    "merged_encoded = pd.DataFrame(one_hot_encoded_data.toarray(), columns=column_names)\n",
    "\n",
    "# Concatenate the original dataframe with the one-hot encoded data\n",
    "merged_encoded = pd.concat([merged_copy, merged_encoded], axis=1)\n",
    "\n",
    "# Drop the original categorical columns\n",
    "merged_encoded.drop(categorical_features, axis=1, inplace=True)\n",
    "\n",
    "# Print the head of the encoded DataFrame\n",
    "merged_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these, we can proceed to modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 5. Modeling\n",
    "\n",
    "### 5.1. Train-Validation Split\n",
    "---\n",
    "\n",
    "Since we are dealing with time series, we can't shuffle our dataset because we have time dependency.  \n",
    "For our project we will use:\n",
    "*   years of `2013`, `2014`, `2015` and `2016` for **training**;\n",
    "*   year of `2017` for **testing**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Using Last Year as Validation Set:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split merged_encoded in train and validation\n",
    "train_set = merged_encoded.loc[merged_encoded['year'].isin([2013, 2014, 2015, 2016])]\n",
    "val_set = merged_encoded.loc[merged_encoded['year'] == 2017]\n",
    "\n",
    "# Separate X, y for training\n",
    "X_train = train_set.drop('sales', axis=1)\n",
    "y_train = train_set['sales'] \n",
    "\n",
    "# Separate X, y for validation\n",
    "X_val = val_set.drop('sales', axis=1)  \n",
    "y_val = val_set['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Using Only last 15 days as Validation Set (Equal to the test size):\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First attempt:\n",
    "# 0.33783152786804155\n",
    "\n",
    "# Second attempt:\n",
    "# 0.3385220432493442\n",
    "\n",
    "# Third attempt\n",
    "# 0.3377147108765348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag features\n",
    "test = merged.copy()\n",
    "\n",
    "for lag in [1, 7, 14, 21, 28]:\n",
    "    test[f'lag_{lag}'] = test.groupby(['store_nbr', 'family'])['sales'].shift(lag)\n",
    "\n",
    "test = create_one_hot(test, [\"family\", \"city\"])\n",
    "test = test.dropna()\n",
    "\n",
    "# Creating feature to check if it is January 1\n",
    "test.set_index('date', inplace=True)\n",
    "test = test[test.index.year == 2017]\n",
    "test['is_january_1'] = (test.index.dayofyear == 1)\n",
    "\n",
    "# Date features:\n",
    "test['year'] = test.index.year\n",
    "test['month'] = test.index.month\n",
    "test['day'] = test.index.day\n",
    "\n",
    "# Dropping some unnused columns\n",
    "columns_to_drop = ['id', 'store_type', 'state']\n",
    "test = test.drop(columns=columns_to_drop)\n",
    "\n",
    "# Creating masks for train/val sets\n",
    "last_15_days = test.index.unique()[-15:]\n",
    "validation_mask = test.index.isin(last_15_days)\n",
    "\n",
    "# Creating train/val sets\n",
    "train_set = test[~validation_mask]\n",
    "val_set = test[validation_mask]\n",
    "\n",
    "# Separate X, y for training\n",
    "X_train = train_set.drop('sales', axis=1)\n",
    "y_train = train_set['sales']\n",
    "\n",
    "# Separate X, y for validation\n",
    "X_val = val_set.drop('sales', axis=1)  \n",
    "y_val = val_set['sales']\n",
    "\n",
    "# numerical_features = ['onpromotion', 'transactions', 'oil_price', 'lag_1', 'lag_7', 'lag_14', 'lag_21', 'lag_28']\n",
    "\n",
    "# # Using StandardScaler for X\n",
    "# scaler_X = StandardScaler()\n",
    "# X_train[numerical_features] = scaler_X.fit_transform(X_train[numerical_features])\n",
    "# X_val[numerical_features] = scaler_X.transform(X_val[numerical_features])\n",
    "\n",
    "# # Using StandardScaler for y\n",
    "# scaler_y = StandardScaler()\n",
    "# y_train = scaler_y.fit_transform(y_train.values.reshape(-1,1))\n",
    "# y_train = pd.Series(y_train.flatten())\n",
    "# y_val = scaler_y.transform(y_val.values.reshape(-1,1))\n",
    "# y_val = pd.Series(y_val.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "model_1 = LinearRegression()\n",
    "model_1.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_1.predict(X_val).clip(0.)\n",
    "\n",
    "# Calculate metrics\n",
    "model_1_mse = mean_squared_error(y_val, y_pred)\n",
    "model_1_mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "model_1_rmsle = np.sqrt(mean_squared_log_error(y_val, y_pred))\n",
    "\n",
    "# Create a DataFrame to store results for Linear Regression\n",
    "results_model_1 = pd.DataFrame({'Model': ['Linear Regression'],\n",
    "                            'RMSLE': [model_1_rmsle],\n",
    "                            'RMSE': [np.sqrt(model_1_mse)],\n",
    "                            'MSE': [model_1_mse],\n",
    "                            'MAE': [model_1_mae]}).round(2)\n",
    "\n",
    "# Print the results_model_1 dataframe\n",
    "results_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_1.predict(X_val)\n",
    "\n",
    "# # Calculate metrics\n",
    "# model_1_mse = mean_squared_error(y_val, y_pred)\n",
    "# model_1_mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "# # Apply the absolute value function to both y_val and y_pred\n",
    "# y_val_abs = abs(y_val)\n",
    "# y_pred_abs = abs(y_pred)\n",
    "\n",
    "# # Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "# model_1_rmsle = np.sqrt(mean_squared_log_error(y_val_abs, y_pred_abs))\n",
    "\n",
    "# # Create a DataFrame to store results for Linear Regression\n",
    "# results_model_1 = pd.DataFrame({'Model': ['Linear Regression'],\n",
    "#                             'RMSLE': [model_1_rmsle],\n",
    "#                             'RMSE': [np.sqrt(model_1_mse)],\n",
    "#                             'MSE': [model_1_mse],\n",
    "#                             'MAE': [model_1_mae]}).round(2)\n",
    "\n",
    "# # Print the results_model_1 dataframe\n",
    "# results_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t1 = pd.Series(scaler_y.inverse_transform(y_pred.reshape(-1, 1)).flatten()).clip(0.)\n",
    "# t2 = pd.Series(scaler_y.inverse_transform(y_val.values.reshape(-1, 1)).flatten())\n",
    "\n",
    "# # Calculate metrics\n",
    "# model_1_mse = mean_squared_error(t2, t1)\n",
    "# model_1_mae = mean_absolute_error(t2, t1)\n",
    "\n",
    "# # Apply the absolute value function to both t2 and t1\n",
    "# t2_abs = abs(t2)\n",
    "# t1_abs = abs(t1)\n",
    "\n",
    "# # Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "# model_1_rmsle = np.sqrt(mean_squared_log_error(t2_abs, t1_abs))\n",
    "\n",
    "# # Create a DataFrame to store results for Linear Regression\n",
    "# results_model_1 = pd.DataFrame({'Model': ['Linear Regression'],\n",
    "#                             'RMSLE': [model_1_rmsle],\n",
    "#                             'RMSE': [np.sqrt(model_1_mse)],\n",
    "#                             'MSE': [model_1_mse],\n",
    "#                             'MAE': [model_1_mae]}).round(2)\n",
    "\n",
    "# # Print the results_model_1 dataframe\n",
    "# results_model_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Model 1: Linear Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "model_1 = LinearRegression()\n",
    "model_1.fit(X_train, y_train)\n",
    "model_1_predictions = model_1.predict(X_val)\n",
    "\n",
    "# Calculate metrics\n",
    "model_1_mse = mean_squared_error(y_val, model_1_predictions)\n",
    "model_1_mae = mean_absolute_error(y_val, model_1_predictions)\n",
    "\n",
    "# Apply the absolute value function to both y_val and model_1_predictions\n",
    "y_val_abs = abs(y_val)\n",
    "model_1_predictions_abs = abs(model_1_predictions)\n",
    "\n",
    "# Calculate the Root Mean Squared Logarithmic Error (RMSLE)\n",
    "model_1_rmsle = np.sqrt(mean_squared_log_error(y_val_abs, model_1_predictions_abs))\n",
    "\n",
    "# Create a DataFrame to store results for Linear Regression\n",
    "results_model_1 = pd.DataFrame({'Model': ['Linear Regression'],\n",
    "                            'RMSLE': [model_1_rmsle],\n",
    "                            'RMSE': [np.sqrt(model_1_mse)],\n",
    "                            'MSE': [model_1_mse],\n",
    "                            'MAE': [model_1_mae]}).round(2)\n",
    "\n",
    "# Print the results_model_1 dataframe\n",
    "results_model_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = model_1.coef_\n",
    "\n",
    "# Step 3: Calculate feature importance\n",
    "importance = np.abs(coefficients)\n",
    "\n",
    "# Optional: Sort features by importance\n",
    "feature_names = [str(name) for name in X_train.columns]\n",
    "sorted_indices = np.argsort(importance)[::-1]\n",
    "sorted_feature_names = np.array(feature_names)[sorted_indices]\n",
    "sorted_importance = importance[sorted_indices]\n",
    "\n",
    "# Print feature importance\n",
    "for feature, imp in zip(sorted_feature_names, sorted_importance):\n",
    "    print(f\"{feature}: {imp:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST SET:\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = merged.copy()\n",
    "test2.set_index('date', inplace=True)\n",
    "\n",
    "for lag in [1, 7, 14, 21, 28]:\n",
    "    test2[f'lag_{lag}'] = test2.groupby(['store_nbr', 'family'])['sales'].shift(lag)\n",
    "test2 = test2.dropna()\n",
    "test2['is_january_1'] = (test2.index.dayofyear == 1)\n",
    "\n",
    "features = ['onpromotion', 'transactions',\n",
    "            'oil_price', 'holiday',\n",
    "            'lag_1', 'lag_7', 'lag_14',\n",
    "            'lag_21', 'lag_28', 'is_january_1', 'sales']\n",
    "\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "# Get unique combinations of store numbers and product families\n",
    "store_product_combinations = test2[['store_nbr', 'family']].drop_duplicates()\n",
    "\n",
    "# Loop through each combination\n",
    "for _, row in store_product_combinations.iterrows():\n",
    "    store = row['store_nbr']\n",
    "    product = row['family']\n",
    "    \n",
    "    # Filter the data for the current store and product family\n",
    "    subset = test2[(test2['store_nbr'] == store) & (test2['family'] == product)]\n",
    "    \n",
    "    # Creating masks for train/val sets\n",
    "    last_15_days = subset.index.unique()[-15:]\n",
    "    validation_mask = subset.index.isin(last_15_days)\n",
    "\n",
    "    # Creating train/val sets\n",
    "    train_set = subset.loc[~validation_mask, features]\n",
    "    val_set = subset.loc[validation_mask, features]\n",
    "\n",
    "    # Separate X, y for training\n",
    "    X_train = train_set.drop('sales', axis=1)\n",
    "    y_train = train_set['sales']\n",
    "\n",
    "    # Separate X, y for validation\n",
    "    X_val = val_set.drop('sales', axis=1)  \n",
    "    y_val = val_set['sales']\n",
    "    \n",
    "    # Train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test set and evaluate\n",
    "    y_pred = model.predict(X_val).clip(0.)\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y_val, y_pred))\n",
    "\n",
    "    # Store the model and its performance\n",
    "    models[(store, product)] = model\n",
    "    results[(store, product)] = rmsle\n",
    "\n",
    "# Now models and results dictionaries hold the models and their performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test3 = merged.copy()\n",
    "test3.set_index('date', inplace=True)\n",
    "\n",
    "for lag in [1, 7, 14, 21, 28]:\n",
    "    test3[f'lag_{lag}'] = test3.groupby(['store_nbr', 'family'])['sales'].shift(lag)\n",
    "test3 = test3.dropna()\n",
    "test3['is_january_1'] = (test3.index.dayofyear == 1)\n",
    "\n",
    "features = ['onpromotion', 'transactions',\n",
    "            'oil_price', 'holiday',\n",
    "            'lag_1', 'lag_7', 'lag_14',\n",
    "            'lag_21', 'lag_28', 'is_january_1', 'sales']\n",
    "\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "test4 = test3.copy()\n",
    "\n",
    "# Get unique combinations of store numbers and product families\n",
    "store_product_combinations = test3[['store_nbr', 'family']].drop_duplicates()\n",
    "\n",
    "# Loop through each combination\n",
    "for _, row in store_product_combinations.iterrows():\n",
    "    store = row['store_nbr']\n",
    "    product = row['family']\n",
    "    \n",
    "    # Filter the data for the current store and product family\n",
    "    subset = test3.loc[(test3['store_nbr'] == store) & (test3['family'] == product), features]\n",
    "\n",
    "    # Separate X, y\n",
    "    X = subset.drop('sales', axis=1)\n",
    "    y = subset['sales']\n",
    "    \n",
    "    # Train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Predict on the test set and evaluate\n",
    "    y_pred = model.predict(X).clip(0.)\n",
    "    rmsle = np.sqrt(mean_squared_log_error(y, y_pred))\n",
    "\n",
    "    # Store the model and its performance\n",
    "    models[(store, product)] = model\n",
    "    results[(store, product)] = rmsle\n",
    "\n",
    "    test4.loc[(test4['store_nbr'] == store) & (test4['family'] == product), 'predicted_sales'] = y_pred\n",
    "\n",
    "# Now models and results dictionaries hold the models and their performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test4[['sales', 'predicted_sales']]\n",
    "\n",
    "np.sqrt(mean_squared_log_error(test4['sales'], test4['predicted_sales']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "test_set = pd.read_csv('./data/test.csv', dtype={'date':'str'}, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>family</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>store_type</th>\n",
       "      <th>cluster</th>\n",
       "      <th>transactions</th>\n",
       "      <th>holiday</th>\n",
       "      <th>oil_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-16</th>\n",
       "      <td>3000888</td>\n",
       "      <td>1</td>\n",
       "      <td>AUTOMOTIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-16</th>\n",
       "      <td>3000889</td>\n",
       "      <td>1</td>\n",
       "      <td>BABY CARE</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-16</th>\n",
       "      <td>3000890</td>\n",
       "      <td>1</td>\n",
       "      <td>BEAUTY</td>\n",
       "      <td>2</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-16</th>\n",
       "      <td>3000891</td>\n",
       "      <td>1</td>\n",
       "      <td>BEVERAGES</td>\n",
       "      <td>20</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-16</th>\n",
       "      <td>3000892</td>\n",
       "      <td>1</td>\n",
       "      <td>BOOKS</td>\n",
       "      <td>0</td>\n",
       "      <td>Quito</td>\n",
       "      <td>Pichincha</td>\n",
       "      <td>D</td>\n",
       "      <td>13</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>46.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  store_nbr      family  onpromotion   city      state  \\\n",
       "date                                                                        \n",
       "2017-08-16  3000888          1  AUTOMOTIVE            0  Quito  Pichincha   \n",
       "2017-08-16  3000889          1   BABY CARE            0  Quito  Pichincha   \n",
       "2017-08-16  3000890          1      BEAUTY            2  Quito  Pichincha   \n",
       "2017-08-16  3000891          1   BEVERAGES           20  Quito  Pichincha   \n",
       "2017-08-16  3000892          1       BOOKS            0  Quito  Pichincha   \n",
       "\n",
       "           store_type  cluster  transactions  holiday  oil_price  \n",
       "date                                                              \n",
       "2017-08-16          D       13           0.0      0.0       46.8  \n",
       "2017-08-16          D       13           0.0      0.0       46.8  \n",
       "2017-08-16          D       13           0.0      0.0       46.8  \n",
       "2017-08-16          D       13           0.0      0.0       46.8  \n",
       "2017-08-16          D       13           0.0      0.0       46.8  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merging with columns ('store_nbr' and 'date') or only ('date') in the datasets using the merge() function with how= 'inner' or 'left', depending.\n",
    "\n",
    "# stores -> train\n",
    "merged_test = pd.merge(left=test_set, right=stores, on='store_nbr', how='inner')\n",
    "\n",
    "# transactions -> train\n",
    "merged_test = pd.merge(left=merged_test, right=transactions, on=['date', 'store_nbr'], how='left').fillna(0)\n",
    "\n",
    "# national_holidays -> train\n",
    "for date in national_holidays['date']:\n",
    "    merged_test.loc[merged_test['date'] == date, 'holiday'] = 1\n",
    "\n",
    "# regional_holidays -> train\n",
    "for date, state in zip(regional_holidays['date'], regional_holidays['locale_name']):\n",
    "    merged_test.loc[(merged_test['date'] == date) & (merged_test['state'] == state), 'holiday'] = 1\n",
    "\n",
    "# local_holidays -> train\n",
    "for date, city in zip(local_holidays['date'], local_holidays['locale_name']):\n",
    "    merged_test.loc[(merged_test['date'] == date) & (merged_test['city'] == city), 'holiday'] = 1\n",
    "\n",
    "# Filling other days of 'holiday' feature with 0\n",
    "merged_test['holiday'] = merged_test['holiday'].fillna(0)\n",
    "\n",
    "# oil -> train\n",
    "merged_test = pd.merge(left=merged_test, right=oil, on='date', how='left')\n",
    "\n",
    "# View the first five rows of the merged_test dataset\n",
    "merged_test.set_index('date', inplace=True)\n",
    "merged_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_concat = pd.concat([merged, merged_test], axis=0)\n",
    "temp_concat.set_index('date', inplace=True)\n",
    "for lag in [1, 7, 14, 21, 28]:\n",
    "    temp_concat[f'lag_{lag}'] = temp_concat.groupby(['store_nbr', 'family'])['sales'].shift(lag)\n",
    "\n",
    "temp_concat['is_january_1'] = (temp_concat.index.dayofyear == 1)\n",
    "temp_concat = temp_concat.fillna(0.)\n",
    "temp_concat = temp_concat.drop('sales', axis=1)\n",
    "\n",
    "test_preds = temp_concat[temp_concat.index > '2017-08-15']\n",
    "\n",
    "# Create a new column in the original DataFrame for the predictions\n",
    "test_preds['sales'] = None\n",
    "\n",
    "# Loop through each unique combination of store_number and product_family\n",
    "store_product_combinations = test_preds[['store_nbr', 'family']].drop_duplicates()\n",
    "\n",
    "features_test = ['onpromotion', 'transactions', 'oil_price', 'holiday', 'lag_1', 'lag_7', 'lag_14', 'lag_21', 'lag_28', 'is_january_1']\n",
    "\n",
    "for _, row in store_product_combinations.iterrows():\n",
    "    store = row['store_nbr']\n",
    "    product = row['family']\n",
    "    \n",
    "    # Filter the data for the current store and product family\n",
    "    subset = test_preds[(test_preds['store_nbr'] == store) & (test_preds['family'] == product)]\n",
    "    \n",
    "    # Prepare your features\n",
    "    X_test = subset[features_test]  # Replace with actual feature columns\n",
    "    \n",
    "    # Load the corresponding model from the models dictionary\n",
    "    model = models[(store, product)]\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_test).clip(0.)\n",
    "    \n",
    "    # Store the predictions back in the original DataFrame\n",
    "    test_preds.loc[(test_preds['store_nbr'] == store) & (test_preds['family'] == product), 'sales'] = predictions\n",
    "\n",
    "# Now test_preds contains the original data along with the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds['id'] = test_preds['id'].astype(np.int32)\n",
    "test_preds['sales'] = test_preds['sales'].astype(np.float64)\n",
    "\n",
    "test_preds = test_preds.sort_values('id', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds[['id', 'sales']].reset_index(drop=True).to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying Different Strategy:\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_set = merged.reset_index()[['date', 'store_nbr', 'family', 'sales']]\n",
    "new_set.set_index(['date', 'store_nbr', 'family'], inplace=True)\n",
    "\n",
    "new_set = new_set.unstack(['store_nbr', 'family'])\n",
    "new_set.index = new_set.index.to_period('D')\n",
    "\n",
    "new_set.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourier = CalendarFourier(\n",
    "    freq='M',\n",
    "    order=4\n",
    ")\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "    index=new_set.index,\n",
    "    constant=True,\n",
    "    order=1,\n",
    "    seasonal=True,\n",
    "    additional_terms=[fourier],\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "X_set = dp.in_sample()\n",
    "\n",
    "X_set['NewYear'] = (X_set.index.dayofyear == 1)\n",
    "\n",
    "X_set.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_new = LinearRegression(fit_intercept=False)\n",
    "model_new.fit(X_set, new_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = [(v[1], v[2]) for v in new_set.columns]\n",
    "# coefficients = model_new.coef_\n",
    "# pd.DataFrame(data=coefficients.T, columns=names, index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "test = pd.read_csv('./data/test.csv', dtype={'date':'str'}, parse_dates=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging with columns ('store_nbr' and 'date') or only ('date') in the datasets using the merge() function with how= 'inner' or 'left', depending.\n",
    "\n",
    "# stores -> train\n",
    "merged_test = pd.merge(left=test_set, right=stores, on='store_nbr', how='inner')\n",
    "\n",
    "# transactions -> train\n",
    "merged_test = pd.merge(left=merged_test, right=transactions, on=['date', 'store_nbr'], how='left').fillna(0)\n",
    "\n",
    "# national_holidays -> train\n",
    "for date in national_holidays['date']:\n",
    "    merged_test.loc[merged_test['date'] == date, 'holiday'] = 1\n",
    "\n",
    "# regional_holidays -> train\n",
    "for date, state in zip(regional_holidays['date'], regional_holidays['locale_name']):\n",
    "    merged_test.loc[(merged_test['date'] == date) & (merged_test['state'] == state), 'holiday'] = 1\n",
    "\n",
    "# local_holidays -> train\n",
    "for date, city in zip(local_holidays['date'], local_holidays['locale_name']):\n",
    "    merged_test.loc[(merged_test['date'] == date) & (merged_test['city'] == city), 'holiday'] = 1\n",
    "\n",
    "# Filling other days of 'holiday' feature with 0\n",
    "merged_test['holiday'] = merged_test['holiday'].fillna(0)\n",
    "\n",
    "# oil -> train\n",
    "merged_test = pd.merge(left=merged_test, right=oil, on='date', how='left')\n",
    "\n",
    "# View the first five rows of the merged_test dataset\n",
    "merged_test.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "results = {}\n",
    "y_set = new_set.copy()\n",
    "temp_concat = pd.concat([merged, merged_test], axis=0)\n",
    "\n",
    "last_15_days = temp_concat.index.unique()[-16:]\n",
    "\n",
    "# Get unique combinations of store numbers and product families\n",
    "store_product_combinations = y_set.stack(['store_nbr', 'family']).reset_index(['store_nbr', 'family'])[['store_nbr', 'family']].drop_duplicates()\n",
    "\n",
    "predictions = pd.DataFrame()\n",
    "# Loop through each combination\n",
    "for _, row in store_product_combinations.iterrows():\n",
    "    store = row['store_nbr']\n",
    "    product = row['family']\n",
    "    \n",
    "    # Filter the data for the current store and product family\n",
    "    sets = temp_concat.loc[(temp_concat['store_nbr'] == store) & (temp_concat['family'] == product), :]\n",
    "    mask = sets.index.isin(last_15_days)\n",
    "\n",
    "    sales_lags = utils.make_lags(sets['sales'], [1, 7, 14, 28], 'sales').fillna(0.)\n",
    "    onpromotion_leads = utils.make_leads(sets['onpromotion'], 3, 'onpromotion').fillna(0.)\n",
    "    onpromotion_lags = utils.make_lags(sets['onpromotion'], [1, 7, 14, 28], 'onpromotion').fillna(0.)\n",
    "    subset = sets[['id', 'holiday', 'transactions']]\n",
    "    subset = pd.concat([subset, sales_lags, onpromotion_leads, onpromotion_lags], axis=1)\n",
    "    subset.index = subset.index.to_period('D')\n",
    "\n",
    "    train_set = subset[~mask]\n",
    "    train_set = pd.concat([X_set, train_set], axis=1).drop('id', axis=1)\n",
    "\n",
    "    X_test = dp.out_of_sample(16)\n",
    "    X_test['NewYear'] = (X_test.index.dayofyear == 1)\n",
    "    \n",
    "    test_set = subset[mask]\n",
    "    test_set = pd.concat([X_test, test_set], axis=1)\n",
    "    test_ids = test_set.pop('id')\n",
    "\n",
    "    # Separate X, y\n",
    "    X_train = train_set.copy()\n",
    "    y = new_set.loc[:, ('sales', store, product)]\n",
    "    \n",
    "    # Train the model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y)\n",
    "    \n",
    "    # Predict on the test set and evaluate\n",
    "    y_pred = model.predict(test_set).clip(0.)\n",
    "    #rmsle = np.sqrt(mean_squared_log_error(y, y_pred))\n",
    "\n",
    "    # Store the model and its performance\n",
    "    models[(store, product)] = model\n",
    "    #results[(store, product)] = rmsle\n",
    "    \n",
    "    df_pred = pd.DataFrame(data={'ids':test_ids, 'sales':y_pred})\n",
    "\n",
    "    predictions = pd.concat([predictions, df_pred], axis=0)\n",
    "    #test4.loc[(test4['store_nbr'] == store) & (test4['family'] == product), 'predicted_sales'] = y_pred\n",
    "\n",
    "predictions = predictions.sort_values('ids', ascending=True).rename(columns={'ids':'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Attempt:\n",
    "# 0.8787151837805383\n",
    "\n",
    "# Second Attempt:\n",
    "# 0.856627329769485\n",
    "\n",
    "# Third Attempt:\n",
    "# 0.6952498252893264\n",
    "\n",
    "np.array([r for k, r in results.items()]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_concat = pd.concat([merged, merged_test], axis=0)\n",
    "for lag in [1, 7, 14, 21, 28]:\n",
    "    temp_concat[f'lag_{lag}'] = temp_concat.groupby(['store_nbr', 'family'])['sales'].shift(lag)\n",
    "\n",
    "temp_concat['is_january_1'] = (temp_concat.index.dayofyear == 1)\n",
    "temp_concat = temp_concat.fillna(0.)\n",
    "\n",
    "temp_concat = pd.concat([temp_concat, utils.make_leads(temp_concat.groupby(['store_nbr', 'family'])['onpromotion'], 3).fillna(0.)], axis=1)\n",
    "temp_concat = temp_concat.reset_index().set_index(['store_nbr','family','date']).sort_index(level='family')\n",
    "temp_concat.index = temp_concat.index.set_levels([temp_concat.index.levels[0], temp_concat.index.levels[1], temp_concat.index.levels[2].to_period('D')])\n",
    "temp_concat = temp_concat.sort_index(level=['store_nbr', 'family'])\n",
    "\n",
    "features = ['transactions', 'holiday', 'oil_price', 'lag_1', 'lag_7', 'lag_14',\n",
    "       'lag_21', 'lag_28', 'y_lead_2', 'y_lead_1', 'y_lead_0']\n",
    "\n",
    "train_temp_concat = temp_concat.loc[temp_concat.index.get_level_values(2) <= '2017-08-15', features]\n",
    "test_temp_concat = temp_concat.loc[temp_concat.index.get_level_values(2) > '2017-08-15', features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_combinations = [(k[1], k[2]) for k in new_set.columns.unique()]\n",
    "expanded_X = pd.concat([X_set] * len(unique_combinations), keys=unique_combinations, names=['store_nbr', 'family'])\n",
    "\n",
    "expanded_X = expanded_X.sort_index(level=['store_nbr', 'family'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded_X_final = pd.merge(left=expanded_X, right=train_temp_concat, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearRegression(fit_intercept=False)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearRegression</label><div class=\"sk-toggleable__content\"><pre>LinearRegression(fit_intercept=False)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearRegression(fit_intercept=False)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trying = expanded_X_final.unstack(['store_nbr','family'])\n",
    "\n",
    "model_try = LinearRegression(fit_intercept=False)\n",
    "model_try.fit(trying, new_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One More model...\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set\n",
    "test_set = pd.read_csv('./data/test.csv', dtype={'date':'str'}, parse_dates=['date'])\n",
    "\n",
    "# Merging with columns ('store_nbr' and 'date') or only ('date') in the datasets using the merge() function with how= 'inner' or 'left', depending.\n",
    "\n",
    "# stores -> train/test\n",
    "merged_train = pd.merge(left=train, right=stores, on='store_nbr', how='inner')\n",
    "merged_test = pd.merge(left=test_set, right=stores, on='store_nbr', how='inner')\n",
    "\n",
    "# transactions -> train/test\n",
    "merged_train = pd.merge(left=merged_train, right=transactions, on=['date', 'store_nbr'], how='left').fillna(0)\n",
    "merged_test = pd.merge(left=merged_test, right=transactions, on=['date', 'store_nbr'], how='left').fillna(0)\n",
    "\n",
    "# national_holidays -> train/test\n",
    "for date in national_holidays['date']:\n",
    "    merged_train.loc[merged_train['date'] == date, 'holiday'] = 1\n",
    "for date in national_holidays['date']:\n",
    "    merged_test.loc[merged_test['date'] == date, 'holiday'] = 1\n",
    "\n",
    "# regional_holidays -> train/test\n",
    "for date, state in zip(regional_holidays['date'], regional_holidays['locale_name']):\n",
    "    merged_train.loc[(merged_train['date'] == date) & (merged_train['state'] == state), 'holiday'] = 1\n",
    "for date, state in zip(regional_holidays['date'], regional_holidays['locale_name']):\n",
    "    merged_test.loc[(merged_test['date'] == date) & (merged_test['state'] == state), 'holiday'] = 1\n",
    "\n",
    "# local_holidays -> train/test\n",
    "for date, city in zip(local_holidays['date'], local_holidays['locale_name']):\n",
    "    merged_train.loc[(merged_train['date'] == date) & (merged_train['city'] == city), 'holiday'] = 1\n",
    "for date, city in zip(local_holidays['date'], local_holidays['locale_name']):\n",
    "    merged_test.loc[(merged_test['date'] == date) & (merged_test['city'] == city), 'holiday'] = 1\n",
    "\n",
    "# Filling other days of 'holiday' feature with 0\n",
    "merged_train['holiday'] = merged_train['holiday'].fillna(0)\n",
    "merged_test['holiday'] = merged_test['holiday'].fillna(0)\n",
    "\n",
    "# oil -> train/test\n",
    "merged_train = pd.merge(left=merged_train, right=oil, on='date', how='left')\n",
    "merged_test = pd.merge(left=merged_test, right=oil, on='date', how='left')\n",
    "\n",
    "# View the first five rows of the merged_train dataset\n",
    "\n",
    "merged_train.set_index('date', inplace=True)\n",
    "merged_test.set_index('date', inplace=True)\n",
    "\n",
    "# Concatenating both dataframes for future use\n",
    "temp_concat = pd.concat([merged_train, merged_test], axis=0)\n",
    "\n",
    "test_set['date'] = test_set['date'].dt.to_period('D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"21\" halign=\"left\">sales</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>store_nbr</th>\n",
       "      <th colspan=\"10\" halign=\"left\">1</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">9</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family</th>\n",
       "      <th>AUTOMOTIVE</th>\n",
       "      <th>BABY CARE</th>\n",
       "      <th>BEAUTY</th>\n",
       "      <th>BEVERAGES</th>\n",
       "      <th>BOOKS</th>\n",
       "      <th>BREAD/BAKERY</th>\n",
       "      <th>CELEBRATION</th>\n",
       "      <th>CLEANING</th>\n",
       "      <th>DAIRY</th>\n",
       "      <th>DELI</th>\n",
       "      <th>...</th>\n",
       "      <th>MAGAZINES</th>\n",
       "      <th>MEATS</th>\n",
       "      <th>PERSONAL CARE</th>\n",
       "      <th>PET SUPPLIES</th>\n",
       "      <th>PLAYERS AND ELECTRONICS</th>\n",
       "      <th>POULTRY</th>\n",
       "      <th>PREPARED FOODS</th>\n",
       "      <th>PRODUCE</th>\n",
       "      <th>SCHOOL AND OFFICE SUPPLIES</th>\n",
       "      <th>SEAFOOD</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1091.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>470.652</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1060.0</td>\n",
       "      <td>579.0</td>\n",
       "      <td>164.069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>374.531</td>\n",
       "      <td>482.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>651.292</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 1782 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                sales                                                \\\n",
       "store_nbr           1                                                 \n",
       "family     AUTOMOTIVE BABY CARE BEAUTY BEVERAGES BOOKS BREAD/BAKERY   \n",
       "date                                                                  \n",
       "2013-01-01        0.0       0.0    0.0       0.0   0.0        0.000   \n",
       "2013-01-02        2.0       0.0    2.0    1091.0   0.0      470.652   \n",
       "\n",
       "                                                 ...                     \\\n",
       "store_nbr                                        ...         9            \n",
       "family     CELEBRATION CLEANING  DAIRY     DELI  ... MAGAZINES    MEATS   \n",
       "date                                             ...                      \n",
       "2013-01-01         0.0      0.0    0.0    0.000  ...       0.0    0.000   \n",
       "2013-01-02         0.0   1060.0  579.0  164.069  ...       0.0  374.531   \n",
       "\n",
       "                                                                        \\\n",
       "store_nbr                                                                \n",
       "family     PERSONAL CARE PET SUPPLIES PLAYERS AND ELECTRONICS  POULTRY   \n",
       "date                                                                     \n",
       "2013-01-01           0.0          0.0                     0.0    0.000   \n",
       "2013-01-02         482.0          0.0                     0.0  651.292   \n",
       "\n",
       "                                                                      \n",
       "store_nbr                                                             \n",
       "family     PREPARED FOODS PRODUCE SCHOOL AND OFFICE SUPPLIES SEAFOOD  \n",
       "date                                                                  \n",
       "2013-01-01            0.0     0.0                        0.0   0.000  \n",
       "2013-01-02           83.0     0.0                        0.0  29.214  \n",
       "\n",
       "[2 rows x 1782 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_set = merged_train[['store_nbr', 'family', 'sales']]\n",
    "y_set = y_set.set_index(['store_nbr', 'family'], append=True)\n",
    "y_set = y_set.unstack(['store_nbr', 'family'])\n",
    "y_set.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>trend</th>\n",
       "      <th>s(2,7)</th>\n",
       "      <th>s(3,7)</th>\n",
       "      <th>s(4,7)</th>\n",
       "      <th>s(5,7)</th>\n",
       "      <th>s(6,7)</th>\n",
       "      <th>s(7,7)</th>\n",
       "      <th>sin(1,freq=M)</th>\n",
       "      <th>cos(1,freq=M)</th>\n",
       "      <th>sin(2,freq=M)</th>\n",
       "      <th>cos(2,freq=M)</th>\n",
       "      <th>sin(3,freq=M)</th>\n",
       "      <th>cos(3,freq=M)</th>\n",
       "      <th>sin(4,freq=M)</th>\n",
       "      <th>cos(4,freq=M)</th>\n",
       "      <th>NewYear</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-02</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201299</td>\n",
       "      <td>0.979530</td>\n",
       "      <td>0.394356</td>\n",
       "      <td>0.918958</td>\n",
       "      <td>0.571268</td>\n",
       "      <td>0.820763</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.688967</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-01-03</th>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.394356</td>\n",
       "      <td>0.918958</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.688967</td>\n",
       "      <td>0.937752</td>\n",
       "      <td>0.347305</td>\n",
       "      <td>0.998717</td>\n",
       "      <td>-0.050649</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            const  trend  s(2,7)  s(3,7)  s(4,7)  s(5,7)  s(6,7)  s(7,7)  \\\n",
       "date                                                                       \n",
       "2013-01-01    1.0    1.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2013-01-02    1.0    2.0     1.0     0.0     0.0     0.0     0.0     0.0   \n",
       "2013-01-03    1.0    3.0     0.0     1.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "            sin(1,freq=M)  cos(1,freq=M)  sin(2,freq=M)  cos(2,freq=M)  \\\n",
       "date                                                                     \n",
       "2013-01-01       0.000000       1.000000       0.000000       1.000000   \n",
       "2013-01-02       0.201299       0.979530       0.394356       0.918958   \n",
       "2013-01-03       0.394356       0.918958       0.724793       0.688967   \n",
       "\n",
       "            sin(3,freq=M)  cos(3,freq=M)  sin(4,freq=M)  cos(4,freq=M)  \\\n",
       "date                                                                     \n",
       "2013-01-01       0.000000       1.000000       0.000000       1.000000   \n",
       "2013-01-02       0.571268       0.820763       0.724793       0.688967   \n",
       "2013-01-03       0.937752       0.347305       0.998717      -0.050649   \n",
       "\n",
       "            NewYear  \n",
       "date                 \n",
       "2013-01-01     True  \n",
       "2013-01-02    False  \n",
       "2013-01-03    False  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fourier = CalendarFourier(\n",
    "    freq='M',\n",
    "    order=4\n",
    ")\n",
    "\n",
    "dp = DeterministicProcess(\n",
    "    index=y_set.index.to_period('D'),\n",
    "    constant=True,\n",
    "    order=1,\n",
    "    seasonal=True,\n",
    "    additional_terms=[fourier],\n",
    "    drop=True\n",
    ")\n",
    "\n",
    "X_set = dp.in_sample()\n",
    "\n",
    "X_set['NewYear'] = (X_set.index.dayofyear == 1)\n",
    "\n",
    "X_set.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5976568420659881\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression(fit_intercept=False, n_jobs=-1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_set, y_set, test_size=30, shuffle=False)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test).clip(0.)\n",
    "\n",
    "rmsle = np.sqrt(mean_squared_log_error(y_test, predictions))\n",
    "print(rmsle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch for Best Hiperparameters in CalendarFourier\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = ['2017', '2016', '2015', '2014', '2013']\n",
    "frequencies = ['D', 'W', 'M', 'Y']\n",
    "orders = [1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "for year in years:\n",
    "\n",
    "    temp_dataset = merged_train.copy()\n",
    "    temp_dataset = temp_dataset.loc[temp_dataset.index >= f'{year}-01-01']\n",
    "    y_set = temp_dataset[['store_nbr', 'family', 'sales']]\n",
    "    y_set = y_set.set_index(['store_nbr', 'family'], append=True)\n",
    "    y_set = y_set.unstack(['store_nbr', 'family'])\n",
    "\n",
    "    for frequency in frequencies:\n",
    "        for order in orders:\n",
    "            fourier = CalendarFourier(\n",
    "                freq=frequency,\n",
    "                order=order\n",
    "            )\n",
    "\n",
    "            dp = DeterministicProcess(\n",
    "                index=y_set.index.to_period('D'),\n",
    "                constant=True,\n",
    "                order=1,\n",
    "                seasonal=True,\n",
    "                additional_terms=[fourier],\n",
    "                drop=True\n",
    "            )\n",
    "\n",
    "            X_set = dp.in_sample()\n",
    "            X_set['NewYear'] = (X_set.index.dayofyear == 1)\n",
    "\n",
    "            model = LinearRegression(fit_intercept=False, n_jobs=-1)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X_set, y_set, test_size=30, shuffle=False)\n",
    "\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            predictions = model.predict(X_test).clip(0.)\n",
    "            rmsle = np.sqrt(mean_squared_log_error(y_test, predictions))\n",
    "\n",
    "            models[(year, frequency, order)] = (model, dp)\n",
    "            results[(year, frequency, order)] = rmsle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2016', 'Y', 2)\n"
     ]
    }
   ],
   "source": [
    "val = 1\n",
    "for result, value in results.items():\n",
    "    if value < val:\n",
    "        val = value\n",
    "        best_result = result\n",
    "\n",
    "print(best_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2017', 'W', 4) 0.542946483275602\n",
      "('2016', 'W', 4) 0.5325073851482142\n",
      "('2015', 'W', 4) 0.5533903366377247\n",
      "('2014', 'W', 4) 0.559179239066486\n",
      "('2013', 'W', 4) 0.571671304849048\n"
     ]
    }
   ],
   "source": [
    "for result, value in results.items():\n",
    "    if (result[1] == 'W') and (result[2] == 4):\n",
    "        print(result, value)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models[('2016', 'W', 4)][1] is the DeterministicProcess used \n",
    "X_submit = models[('2016', 'W', 4)][1].out_of_sample(test_set['date'].unique().shape[0])\n",
    "X_submit['NewYear'] = (X_submit.index.dayofyear == 1)\n",
    "X_submit.index.name = 'date'\n",
    "\n",
    "# models[('2016', 'W', 4)][0] is the LinearRegression model used \n",
    "submit_predictions = models[('2016', 'W', 4)][0].predict(X_submit)\n",
    "submit_predictions = pd.DataFrame(submit_predictions, index=X_submit.index, columns=y_test.columns)\n",
    "\n",
    "submit_predictions = submit_predictions.stack(['store_nbr', 'family'])\n",
    "submit_predictions = submit_predictions.join(test_set.set_index(['date', 'store_nbr', 'family'])['id']).reindex(columns=['id', 'sales'])\n",
    "\n",
    "submit_predictions.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
